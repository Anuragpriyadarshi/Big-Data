package sparkpack
import org.apache.spark.sql.functions._

import org.apache.spark._
import org.apache.spark.sql.functions._
import sys.process._
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._


object sparkobj {

case class schema(txnno:String, txndate:String, custno:String, amount:String, category:String, product:String,city:String, state:String, spendby:String)
def main(args:Array[String]):Unit=

{

		val conf = new SparkConf().setAppName("First").setMaster("local[*]")
				val sc = new SparkContext(conf)

				sc.setLogLevel("ERROR")
				val spark = SparkSession
				.builder()
				.getOrCreate()		

				import spark.implicits._

				val df=spark.read.format("csv").
				option("Header","true")
				.option("delimiter","~")
				.load("file:///C:/Data/txns_head")
				df.show()

				println("==================Processed data======================")

				val procdf = df.filter(col("category")==="Gymnastics")
				.withColumn("txndate", expr("split(txndate,'-')[2]"))
				.withColumnRenamed("txndate","year")
				.withColumn("check",expr("case when spendby='cash' then 0 else 1 end"))
				.withColumn("city",concat(col("city"),lit('-'),
						col("state")))

				.drop("State")
				.withColumnRenamed("city","location")


				procdf.show()
}
}