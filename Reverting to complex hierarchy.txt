package sparkpack
import org.apache.spark.sql.functions._

import org.apache.spark._
import org.apache.spark.sql.functions._
import sys.process._
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._


object sparkobj {

case class schema(txnno:String,txndate:String,custno:String,amount:String,category:String,product:String,city:String,state:String,spendby:String)
def main(args:Array[String]):Unit=

{

		val conf = new SparkConf().setAppName("First").setMaster("local[*]")
				val sc = new SparkContext(conf)

				sc.setLogLevel("ERROR")
				val spark = SparkSession
				.builder()
				.getOrCreate()		

				import spark.implicits._

				val df=spark.read.format("json").option("multiline","true").load("file:///C:/Data/complexjson/zeyoc.json")
				df.show()
				df.printSchema()

				println("==========flatening data====================")

				val fldata=df.select(
						col("orgname"),
						col("trainer"),
						col("address.permanentAddress.location").as("plocation"),
						col("address.permanentAddress.area").as("parea"),
						col("address.temporaryAddress.location").as("tlocation"),
						col("address.temporaryAddress.area").as("tarea")
						)
				fldata.show()
				fldata.printSchema()

				println("==========Reverting data to complex struct====================")

				val compdata=fldata.select(
						col("orgname"),
						col("trainer"),
						struct(
								struct(
										col("parea").as("area"),
										col("plocation").as("location")

										).as("permanentAddress"),
								struct(
										col("tarea").as("area"),
										col("tlocation").as("location")
										).as("temporaryAddress")
								).as("address")
						)

				compdata.show()
				compdata.printSchema()
}

}