package sparkpack

import org.apache.spark._
import sys.process._
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._


object sparkobj {

case class schema(txnno:String, txndate:String, custno:String, amount:String, category:String, product:String,city:String, state:String, spendby:String)
def main(args:Array[String]):Unit=

{

		val conf = new SparkConf().setAppName("First").setMaster("local[*]")
				val sc = new SparkContext(conf)

				sc.setLogLevel("ERROR")
				val spark = SparkSession
				.builder()
				.getOrCreate()		

				import spark.implicits._
				println("======================Raw==================================")

				val df=spark.read.format("csv").option("header","true").load("file:///c:/data/usdata.csv")
				df.show()

				println("======================Read csv data and writing Parquet==================================")

				df.write.format("parquet").mode("overwrite").save("file:///C:/data/parquetdata")
				println("==========================Parquet Written=====================")

				println("======================Read Parquet data and writing JSON==================================")
				val pardf=spark.read.format("parquet").load("file:///C:/data/parquetdata")
				
				pardf.write.format("json").mode("overwrite").save("file:///C:/data/jsondata")
				
				println("==========================JSON Written=====================")
				
				println("======================Read JSON data and writing ORC==================================")
				
				val orcdf=spark.read.format("json").load("file:///C:/data/jsondata")
				orcdf.write.format("orc").mode("overwrite").save("file:///C:/data/orcdata")
				println("==========================ORC Written=====================")
				
				println("======================Read ORC data and writing CSV==================================")
				val CSVdf=spark.read.format("orc").load("file:///C:/data/orcdata")
				CSVdf.write.format("csv").mode("overwrite").save("file:///C:/data/csvdata")
				println("==========================CSV Written=====================")
}
}



================================================================================================================================

package sparkpack

import org.apache.spark._
import sys.process._
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._


object sparkobj {

case class schema(txnno:String, txndate:String, custno:String, amount:String, category:String, product:String,city:String, state:String, spendby:String)
def main(args:Array[String]):Unit=

{

		val conf = new SparkConf().setAppName("First").setMaster("local[*]")
				val sc = new SparkContext(conf)

				sc.setLogLevel("ERROR")
				val spark = SparkSession
				.builder()
				.getOrCreate()		

				import spark.implicits._
				
				val jsdf=spark.read.format("json").option("multiline","true").load("file:///c:/data/random10.json")
				jsdf.show()
				jsdf.printSchema()
}
}



